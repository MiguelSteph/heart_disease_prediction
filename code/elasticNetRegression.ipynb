{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape is (44, 20) and Test labels shape is (44,)\n",
      "Validation data shape is (44, 20) and Validation labels shape is (44,)\n",
      "Train data shape is (209, 20) and Train labels shape is (209,)\n"
     ]
    }
   ],
   "source": [
    "#Load preprocessed dataset\n",
    "savedPath = \"../data/splittedData.pickle\"\n",
    "\n",
    "with open(savedPath, \"rb\") as input_file:\n",
    "    dataDict = pickle.load(input_file)\n",
    "\n",
    "testData = dataDict[\"testData\"]\n",
    "testLabels = dataDict[\"testLabels\"]\n",
    "validationData = dataDict[\"validationData\"]\n",
    "validationLabels = dataDict[\"validationLabels\"]\n",
    "trainData = dataDict[\"trainData\"]\n",
    "trainLabels = dataDict[\"trainLabels\"]\n",
    "\n",
    "testLabels = testLabels.astype(np.float32)\n",
    "validationLabels = validationLabels.astype(np.float32)\n",
    "trainLabels = trainLabels.astype(np.float32)\n",
    "\n",
    "print(\"Test data shape is {} and Test labels shape is {}\".format(testData.shape, testLabels.shape))\n",
    "print(\"Validation data shape is {} and Validation labels shape is {}\"\n",
    "      .format(validationData.shape, validationLabels.shape))\n",
    "print(\"Train data shape is {} and Train labels shape is {}\".format(trainData.shape, trainLabels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL PARAMS\n",
    "NUMBER_OF_FEATURES = 20\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCH = 200\n",
    "CHECK_GAP = 100\n",
    "ALPHA_LASSO = 1.0\n",
    "ALPHA_RIDGE = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "xVals = tf.placeholder(shape=[None, NUMBER_OF_FEATURES], dtype=np.float32)\n",
    "yVals = tf.placeholder(shape=[None, 1], dtype=np.float32)\n",
    "\n",
    "weight = tf.Variable(tf.random_normal(shape=[NUMBER_OF_FEATURES, 1], mean=0.0, stddev=1))\n",
    "biais = tf.Variable(tf.random_normal(shape=[1, 1], mean=0.0, stddev=1))\n",
    "\n",
    "modelOutput = tf.add(tf.matmul(xVals, weight), biais)\n",
    "\n",
    "#Compute the loss\n",
    "weightAbs = tf.reduce_mean(tf.multiply(ALPHA_LASSO, tf.abs(weight)))\n",
    "weightSqrt = tf.reduce_mean(tf.multiply(ALPHA_RIDGE, tf.square(weight)))\n",
    "origLoss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=modelOutput, labels=yVals))\n",
    "loss = tf.add(tf.add(weightAbs, weightSqrt), origLoss)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "trainStep = optimizer.minimize(loss)\n",
    "\n",
    "#Compute Accuracy\n",
    "prediction = tf.round(tf.nn.sigmoid(modelOutput))\n",
    "correctPrediction = tf.cast(tf.equal(prediction, yVals), dtype=np.float32)\n",
    "accuracy = tf.reduce_mean(correctPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 100 VALIDATION: loss = 4.352722644805908 and accuracy = 0.5227272510528564\n",
      "\n",
      "\n",
      "Step 200 VALIDATION: loss = 2.7680273056030273 and accuracy = 0.5454545617103577\n",
      "\n",
      "\n",
      "Step 300 VALIDATION: loss = 1.7606501579284668 and accuracy = 0.5909090638160706\n",
      "\n",
      "\n",
      "Step 400 VALIDATION: loss = 1.235652208328247 and accuracy = 0.6590909361839294\n",
      "\n",
      "\n",
      "Step 500 VALIDATION: loss = 0.9589380025863647 and accuracy = 0.6818181872367859\n",
      "\n",
      "\n",
      "Step 600 VALIDATION: loss = 0.787898063659668 and accuracy = 0.75\n",
      "\n",
      "\n",
      "Step 700 VALIDATION: loss = 0.6869346499443054 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 800 VALIDATION: loss = 0.6236221790313721 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 900 VALIDATION: loss = 0.5945247411727905 and accuracy = 0.75\n",
      "\n",
      "\n",
      "Step 1000 VALIDATION: loss = 0.585159182548523 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 1100 VALIDATION: loss = 0.5805262327194214 and accuracy = 0.8181818127632141\n",
      "\n",
      "\n",
      "Step 1200 VALIDATION: loss = 0.5761866569519043 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1300 VALIDATION: loss = 0.5728492736816406 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1400 VALIDATION: loss = 0.5710736513137817 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1500 VALIDATION: loss = 0.5693917274475098 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1600 VALIDATION: loss = 0.5676195025444031 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1700 VALIDATION: loss = 0.5657972097396851 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1800 VALIDATION: loss = 0.565410852432251 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1900 VALIDATION: loss = 0.5651055574417114 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2000 VALIDATION: loss = 0.5645601749420166 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2100 VALIDATION: loss = 0.5634455680847168 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2200 VALIDATION: loss = 0.5631639957427979 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2300 VALIDATION: loss = 0.5625832676887512 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2400 VALIDATION: loss = 0.5639169216156006 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2500 VALIDATION: loss = 0.5622075796127319 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2600 VALIDATION: loss = 0.562188446521759 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2700 VALIDATION: loss = 0.5628236532211304 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2800 VALIDATION: loss = 0.5619874000549316 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2900 VALIDATION: loss = 0.5625348687171936 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3000 VALIDATION: loss = 0.5623319149017334 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3100 VALIDATION: loss = 0.5616739988327026 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3200 VALIDATION: loss = 0.5625367760658264 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3300 VALIDATION: loss = 0.5617451667785645 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3400 VALIDATION: loss = 0.5624862313270569 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3500 VALIDATION: loss = 0.5629897117614746 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3600 VALIDATION: loss = 0.562229573726654 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3700 VALIDATION: loss = 0.562980592250824 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3800 VALIDATION: loss = 0.5632923245429993 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3900 VALIDATION: loss = 0.5616346597671509 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4000 VALIDATION: loss = 0.5624637603759766 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4100 VALIDATION: loss = 0.5617183446884155 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4200 VALIDATION: loss = 0.5619224309921265 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4300 VALIDATION: loss = 0.5611243844032288 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4400 VALIDATION: loss = 0.5614012479782104 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4500 VALIDATION: loss = 0.5614508390426636 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4600 VALIDATION: loss = 0.5618864893913269 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4700 VALIDATION: loss = 0.5618636608123779 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4800 VALIDATION: loss = 0.5608144402503967 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4900 VALIDATION: loss = 0.5624536275863647 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 5000 VALIDATION: loss = 0.5615796446800232 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 5100 VALIDATION: loss = 0.561997652053833 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 5200 VALIDATION: loss = 0.5614337921142578 and accuracy = 0.8409090638160706\n",
      "\n",
      "ACCURACY ON TEST SET: 0.8409090638160706\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "steps = []\n",
    "lossVals = []\n",
    "accVals = []\n",
    "\n",
    "currentStep = 0\n",
    "trainDataSize = trainData.shape[0]\n",
    "np.random.seed(seed=18)\n",
    "\n",
    "for index in range(NUM_EPOCH) :\n",
    "    indexes = np.arange(trainDataSize)\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    highIndex = 0\n",
    "    while highIndex + BATCH_SIZE < trainDataSize :\n",
    "        batchIndexes = indexes[highIndex:highIndex + BATCH_SIZE]\n",
    "        highIndex = highIndex + BATCH_SIZE\n",
    "        \n",
    "        lss, _, acc = sess.run([loss, trainStep, accuracy], feed_dict={xVals: trainData[batchIndexes], \n",
    "                                                               yVals: trainLabels[batchIndexes].reshape((-1, 1))})    \n",
    "        currentStep = currentStep + 1\n",
    "#         print(\"Step {} Training: loss = {} and accuracy = {}\".format(currentStep, lss, acc))\n",
    "        \n",
    "        if currentStep % CHECK_GAP == 0 :\n",
    "            validationIndexes = np.arange(validationData.shape[0])\n",
    "            np.random.shuffle(validationIndexes)\n",
    "            lossVall, accVall = sess.run([loss, accuracy], feed_dict={xVals: validationData[validationIndexes], \n",
    "                                                               yVals: validationLabels[validationIndexes].reshape((-1, 1))})\n",
    "            steps.append(currentStep)\n",
    "            lossVals.append(lossVall)\n",
    "            accVals.append(accVall)\n",
    "            \n",
    "            print()\n",
    "            print(\"Step {} VALIDATION: loss = {} and accuracy = {}\".format(currentStep, lossVall, accVall))\n",
    "            print()\n",
    "\n",
    "#Accuracy on test set\n",
    "accTest = sess.run(accuracy, feed_dict={xVals: testData, yVals: testLabels.reshape((-1, 1))})\n",
    "print(\"ACCURACY ON TEST SET: {}\".format(accTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model metrics saved\n"
     ]
    }
   ],
   "source": [
    "modelMetricDic = {\"steps\": steps, \n",
    "                  \"lossVals\": lossVals, \n",
    "                  \"accVals\": accVals, \n",
    "                  \"accTest\": accTest}\n",
    "savedPath = \"../data/elasticNetRegressionModelMetrics.pickle\"\n",
    "with open(savedPath, 'wb') as handle:\n",
    "    pickle.dump(modelMetricDic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Model metrics saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
