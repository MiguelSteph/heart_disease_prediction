{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape is (44, 20) and Test labels shape is (44,)\n",
      "Validation data shape is (44, 20) and Validation labels shape is (44,)\n",
      "Train data shape is (209, 20) and Train labels shape is (209,)\n"
     ]
    }
   ],
   "source": [
    "#Load preprocessed dataset\n",
    "savedPath = \"../data/splittedData.pickle\"\n",
    "\n",
    "with open(savedPath, \"rb\") as input_file:\n",
    "    dataDict = pickle.load(input_file)\n",
    "\n",
    "testData = dataDict[\"testData\"]\n",
    "testLabels = dataDict[\"testLabels\"]\n",
    "validationData = dataDict[\"validationData\"]\n",
    "validationLabels = dataDict[\"validationLabels\"]\n",
    "trainData = dataDict[\"trainData\"]\n",
    "trainLabels = dataDict[\"trainLabels\"]\n",
    "\n",
    "testLabels = testLabels.astype(np.float32)\n",
    "validationLabels = validationLabels.astype(np.float32)\n",
    "trainLabels = trainLabels.astype(np.float32)\n",
    "\n",
    "print(\"Test data shape is {} and Test labels shape is {}\".format(testData.shape, testLabels.shape))\n",
    "print(\"Validation data shape is {} and Validation labels shape is {}\"\n",
    "      .format(validationData.shape, validationLabels.shape))\n",
    "print(\"Train data shape is {} and Train labels shape is {}\".format(trainData.shape, trainLabels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL PARAMS\n",
    "NUMBER_OF_FEATURES = 20\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCH = 200\n",
    "CHECK_GAP = 100\n",
    "ALPHA = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "xVals = tf.placeholder(shape=[None, NUMBER_OF_FEATURES], dtype=np.float32)\n",
    "yVals = tf.placeholder(shape=[None, 1], dtype=np.float32)\n",
    "\n",
    "weight = tf.Variable(tf.random_normal(shape=[NUMBER_OF_FEATURES, 1], mean=0.0, stddev=1))\n",
    "biais = tf.Variable(tf.random_normal(shape=[1, 1], mean=0.0, stddev=1))\n",
    "\n",
    "modelOutput = tf.add(tf.matmul(xVals, weight), biais)\n",
    "\n",
    "#Compute the loss\n",
    "weightSqrt = tf.reduce_mean(tf.multiply(ALPHA, tf.square(weight)))\n",
    "origLoss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=modelOutput, labels=yVals))\n",
    "loss = tf.add(weightSqrt, origLoss)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "trainStep = optimizer.minimize(loss)\n",
    "\n",
    "#Compute Accuracy\n",
    "prediction = tf.round(tf.nn.sigmoid(modelOutput))\n",
    "correctPrediction = tf.cast(tf.equal(prediction, yVals), dtype=np.float32)\n",
    "accuracy = tf.reduce_mean(correctPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 100 VALIDATION: loss = 1.2057132720947266 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 200 VALIDATION: loss = 1.0037140846252441 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 300 VALIDATION: loss = 0.8656360507011414 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 400 VALIDATION: loss = 0.7655185461044312 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 500 VALIDATION: loss = 0.6954597234725952 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 600 VALIDATION: loss = 0.6428087949752808 and accuracy = 0.8181818127632141\n",
      "\n",
      "\n",
      "Step 700 VALIDATION: loss = 0.6029884219169617 and accuracy = 0.8181818127632141\n",
      "\n",
      "\n",
      "Step 800 VALIDATION: loss = 0.5727649331092834 and accuracy = 0.8181818127632141\n",
      "\n",
      "\n",
      "Step 900 VALIDATION: loss = 0.5482239127159119 and accuracy = 0.8181818127632141\n",
      "\n",
      "\n",
      "Step 1000 VALIDATION: loss = 0.5308189988136292 and accuracy = 0.8181818127632141\n",
      "\n",
      "\n",
      "Step 1100 VALIDATION: loss = 0.5158265233039856 and accuracy = 0.8181818127632141\n",
      "\n",
      "\n",
      "Step 1200 VALIDATION: loss = 0.5048559904098511 and accuracy = 0.8181818127632141\n",
      "\n",
      "\n",
      "Step 1300 VALIDATION: loss = 0.4960165321826935 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1400 VALIDATION: loss = 0.489174485206604 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1500 VALIDATION: loss = 0.48274046182632446 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1600 VALIDATION: loss = 0.4791623055934906 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1700 VALIDATION: loss = 0.47491973638534546 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1800 VALIDATION: loss = 0.4721336364746094 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1900 VALIDATION: loss = 0.4704952538013458 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2000 VALIDATION: loss = 0.46866095066070557 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2100 VALIDATION: loss = 0.46666479110717773 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2200 VALIDATION: loss = 0.46544599533081055 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2300 VALIDATION: loss = 0.4649274945259094 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2400 VALIDATION: loss = 0.46497535705566406 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2500 VALIDATION: loss = 0.4638186991214752 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2600 VALIDATION: loss = 0.4635682702064514 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2700 VALIDATION: loss = 0.46363475918769836 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2800 VALIDATION: loss = 0.46332699060440063 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2900 VALIDATION: loss = 0.4630333483219147 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3000 VALIDATION: loss = 0.4628429710865021 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3100 VALIDATION: loss = 0.46270740032196045 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3200 VALIDATION: loss = 0.46304911375045776 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3300 VALIDATION: loss = 0.4628630578517914 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3400 VALIDATION: loss = 0.4627934992313385 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3500 VALIDATION: loss = 0.46420854330062866 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3600 VALIDATION: loss = 0.463531494140625 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3700 VALIDATION: loss = 0.4628414213657379 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3800 VALIDATION: loss = 0.46371132135391235 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3900 VALIDATION: loss = 0.4627684950828552 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4000 VALIDATION: loss = 0.4634777009487152 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4100 VALIDATION: loss = 0.46305689215660095 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4200 VALIDATION: loss = 0.4638022184371948 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4300 VALIDATION: loss = 0.4620731472969055 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4400 VALIDATION: loss = 0.46266433596611023 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4500 VALIDATION: loss = 0.4630667269229889 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4600 VALIDATION: loss = 0.4622839093208313 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4700 VALIDATION: loss = 0.46291133761405945 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4800 VALIDATION: loss = 0.46249014139175415 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4900 VALIDATION: loss = 0.4634525775909424 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 5000 VALIDATION: loss = 0.4629121720790863 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 5100 VALIDATION: loss = 0.46333402395248413 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 5200 VALIDATION: loss = 0.46302321553230286 and accuracy = 0.8409090638160706\n",
      "\n",
      "ACCURACY ON TEST SET: 0.8181818127632141\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "steps = []\n",
    "lossVals = []\n",
    "accVals = []\n",
    "\n",
    "currentStep = 0\n",
    "trainDataSize = trainData.shape[0]\n",
    "np.random.seed(seed=18)\n",
    "\n",
    "for index in range(NUM_EPOCH) :\n",
    "    indexes = np.arange(trainDataSize)\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    highIndex = 0\n",
    "    while highIndex + BATCH_SIZE < trainDataSize :\n",
    "        batchIndexes = indexes[highIndex:highIndex + BATCH_SIZE]\n",
    "        highIndex = highIndex + BATCH_SIZE\n",
    "        \n",
    "        lss, _, acc = sess.run([loss, trainStep, accuracy], feed_dict={xVals: trainData[batchIndexes], \n",
    "                                                               yVals: trainLabels[batchIndexes].reshape((-1, 1))})    \n",
    "        currentStep = currentStep + 1\n",
    "#         print(\"Step {} Training: loss = {} and accuracy = {}\".format(currentStep, lss, acc))\n",
    "        \n",
    "        if currentStep % CHECK_GAP == 0 :\n",
    "            validationIndexes = np.arange(validationData.shape[0])\n",
    "            np.random.shuffle(validationIndexes)\n",
    "            lossVall, accVall = sess.run([loss, accuracy], feed_dict={xVals: validationData[validationIndexes], \n",
    "                                                               yVals: validationLabels[validationIndexes].reshape((-1, 1))})\n",
    "            steps.append(currentStep)\n",
    "            lossVals.append(lossVall)\n",
    "            accVals.append(accVall)\n",
    "            \n",
    "            print()\n",
    "            print(\"Step {} VALIDATION: loss = {} and accuracy = {}\".format(currentStep, lossVall, accVall))\n",
    "            print()\n",
    "\n",
    "#Accuracy on test set\n",
    "accTest = sess.run(accuracy, feed_dict={xVals: testData, yVals: testLabels.reshape((-1, 1))})\n",
    "print(\"ACCURACY ON TEST SET: {}\".format(accTest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model metrics saved\n"
     ]
    }
   ],
   "source": [
    "modelMetricDic = {\"steps\": steps, \n",
    "                  \"lossVals\": lossVals, \n",
    "                  \"accVals\": accVals, \n",
    "                  \"accTest\": accTest}\n",
    "savedPath = \"../data/ridgeRegressionModelMetrics.pickle\"\n",
    "with open(savedPath, 'wb') as handle:\n",
    "    pickle.dump(modelMetricDic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Model metrics saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
