{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape is (44, 20) and Test labels shape is (44,)\n",
      "Validation data shape is (44, 20) and Validation labels shape is (44,)\n",
      "Train data shape is (209, 20) and Train labels shape is (209,)\n"
     ]
    }
   ],
   "source": [
    "#Load preprocessed dataset\n",
    "savedPath = \"../data/splittedData.pickle\"\n",
    "\n",
    "with open(savedPath, \"rb\") as input_file:\n",
    "    dataDict = pickle.load(input_file)\n",
    "\n",
    "testData = dataDict[\"testData\"]\n",
    "testLabels = dataDict[\"testLabels\"]\n",
    "validationData = dataDict[\"validationData\"]\n",
    "validationLabels = dataDict[\"validationLabels\"]\n",
    "trainData = dataDict[\"trainData\"]\n",
    "trainLabels = dataDict[\"trainLabels\"]\n",
    "\n",
    "testLabels = testLabels.astype(np.float32)\n",
    "validationLabels = validationLabels.astype(np.float32)\n",
    "trainLabels = trainLabels.astype(np.float32)\n",
    "\n",
    "print(\"Test data shape is {} and Test labels shape is {}\".format(testData.shape, testLabels.shape))\n",
    "print(\"Validation data shape is {} and Validation labels shape is {}\"\n",
    "      .format(validationData.shape, validationLabels.shape))\n",
    "print(\"Train data shape is {} and Train labels shape is {}\".format(trainData.shape, trainLabels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL PARAMS\n",
    "NUMBER_OF_FEATURES = 20\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCH = 200\n",
    "CHECK_GAP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "xVals = tf.placeholder(shape=[None, NUMBER_OF_FEATURES], dtype=np.float32)\n",
    "yVals = tf.placeholder(shape=[None, 1], dtype=np.float32)\n",
    "\n",
    "weight = tf.Variable(tf.random_normal(shape=[NUMBER_OF_FEATURES, 1], mean=0.0, stddev=1))\n",
    "biais = tf.Variable(tf.random_normal(shape=[1, 1], mean=0.0, stddev=1))\n",
    "\n",
    "modelOutput = tf.add(tf.matmul(xVals, weight), biais)\n",
    "\n",
    "#Compute the loss\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=modelOutput, labels=yVals))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "trainStep = optimizer.minimize(loss)\n",
    "\n",
    "#Compute Accuracy\n",
    "prediction = tf.round(tf.nn.sigmoid(modelOutput))\n",
    "correctPrediction = tf.cast(tf.equal(prediction, yVals), dtype=np.float32)\n",
    "accuracy = tf.reduce_mean(correctPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 100 VALIDATION: loss = 1.5826268196105957 and accuracy = 0.40909090638160706\n",
      "\n",
      "\n",
      "Step 200 VALIDATION: loss = 1.0761923789978027 and accuracy = 0.6363636255264282\n",
      "\n",
      "\n",
      "Step 300 VALIDATION: loss = 0.8326372504234314 and accuracy = 0.6590909361839294\n",
      "\n",
      "\n",
      "Step 400 VALIDATION: loss = 0.707487165927887 and accuracy = 0.6590909361839294\n",
      "\n",
      "\n",
      "Step 500 VALIDATION: loss = 0.6230003833770752 and accuracy = 0.6590909361839294\n",
      "\n",
      "\n",
      "Step 600 VALIDATION: loss = 0.5680159330368042 and accuracy = 0.7272727489471436\n",
      "\n",
      "\n",
      "Step 700 VALIDATION: loss = 0.5308763384819031 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 800 VALIDATION: loss = 0.5065802931785583 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 900 VALIDATION: loss = 0.4895879626274109 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 1000 VALIDATION: loss = 0.4789346754550934 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 1100 VALIDATION: loss = 0.4693215489387512 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 1200 VALIDATION: loss = 0.46481797099113464 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 1300 VALIDATION: loss = 0.4619516432285309 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 1400 VALIDATION: loss = 0.4604870676994324 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 1500 VALIDATION: loss = 0.4587364196777344 and accuracy = 0.75\n",
      "\n",
      "\n",
      "Step 1600 VALIDATION: loss = 0.4594978392124176 and accuracy = 0.75\n",
      "\n",
      "\n",
      "Step 1700 VALIDATION: loss = 0.45841744542121887 and accuracy = 0.75\n",
      "\n",
      "\n",
      "Step 1800 VALIDATION: loss = 0.4592175781726837 and accuracy = 0.75\n",
      "\n",
      "\n",
      "Step 1900 VALIDATION: loss = 0.4601256549358368 and accuracy = 0.75\n",
      "\n",
      "\n",
      "Step 2000 VALIDATION: loss = 0.4613046646118164 and accuracy = 0.75\n",
      "\n",
      "\n",
      "Step 2100 VALIDATION: loss = 0.4615843892097473 and accuracy = 0.75\n",
      "\n",
      "\n",
      "Step 2200 VALIDATION: loss = 0.46271467208862305 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 2300 VALIDATION: loss = 0.46455925703048706 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 2400 VALIDATION: loss = 0.46615493297576904 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 2500 VALIDATION: loss = 0.4672117233276367 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 2600 VALIDATION: loss = 0.46855735778808594 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 2700 VALIDATION: loss = 0.46992284059524536 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 2800 VALIDATION: loss = 0.4714622497558594 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 2900 VALIDATION: loss = 0.47249144315719604 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 3000 VALIDATION: loss = 0.4741806983947754 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 3100 VALIDATION: loss = 0.4753454029560089 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 3200 VALIDATION: loss = 0.47670796513557434 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 3300 VALIDATION: loss = 0.4779553711414337 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 3400 VALIDATION: loss = 0.4790653884410858 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 3500 VALIDATION: loss = 0.48183882236480713 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 3600 VALIDATION: loss = 0.4821581542491913 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 3700 VALIDATION: loss = 0.48322248458862305 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 3800 VALIDATION: loss = 0.48473799228668213 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 3900 VALIDATION: loss = 0.48540422320365906 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 4000 VALIDATION: loss = 0.48726558685302734 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 4100 VALIDATION: loss = 0.48789283633232117 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 4200 VALIDATION: loss = 0.48963719606399536 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 4300 VALIDATION: loss = 0.4894537925720215 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 4400 VALIDATION: loss = 0.49100130796432495 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 4500 VALIDATION: loss = 0.49222850799560547 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 4600 VALIDATION: loss = 0.4924621284008026 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 4700 VALIDATION: loss = 0.4936874508857727 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 4800 VALIDATION: loss = 0.49485233426094055 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 4900 VALIDATION: loss = 0.49672430753707886 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 5000 VALIDATION: loss = 0.4970036745071411 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 5100 VALIDATION: loss = 0.49856337904930115 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 5200 VALIDATION: loss = 0.4993974566459656 and accuracy = 0.7954545617103577\n",
      "\n",
      "ACCURACY ON TEST SET: 0.8181818127632141\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "steps = []\n",
    "lossVals = []\n",
    "accVals = []\n",
    "\n",
    "currentStep = 0\n",
    "trainDataSize = trainData.shape[0]\n",
    "np.random.seed(seed=18)\n",
    "\n",
    "for index in range(NUM_EPOCH) :\n",
    "    indexes = np.arange(trainDataSize)\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    highIndex = 0\n",
    "    while highIndex + BATCH_SIZE < trainDataSize :\n",
    "        batchIndexes = indexes[highIndex:highIndex + BATCH_SIZE]\n",
    "        highIndex = highIndex + BATCH_SIZE\n",
    "        \n",
    "        lss, _, acc = sess.run([loss, trainStep, accuracy], feed_dict={xVals: trainData[batchIndexes], \n",
    "                                                               yVals: trainLabels[batchIndexes].reshape((-1, 1))})    \n",
    "        currentStep = currentStep + 1\n",
    "#         print(\"Step {} Training: loss = {} and accuracy = {}\".format(currentStep, lss, acc))\n",
    "        \n",
    "        if currentStep % CHECK_GAP == 0 :\n",
    "            validationIndexes = np.arange(validationData.shape[0])\n",
    "            np.random.shuffle(validationIndexes)\n",
    "            lossVall, accVall = sess.run([loss, accuracy], feed_dict={xVals: validationData[validationIndexes], \n",
    "                                                               yVals: validationLabels[validationIndexes].reshape((-1, 1))})\n",
    "            steps.append(currentStep)\n",
    "            lossVals.append(lossVall)\n",
    "            accVals.append(accVall)\n",
    "            \n",
    "            print()\n",
    "            print(\"Step {} VALIDATION: loss = {} and accuracy = {}\".format(currentStep, lossVall, accVall))\n",
    "            print()\n",
    "\n",
    "#Accuracy on test set\n",
    "accTest = sess.run(accuracy, feed_dict={xVals: testData, yVals: testLabels.reshape((-1, 1))})\n",
    "print(\"ACCURACY ON TEST SET: {}\".format(accTest))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model metrics saved\n"
     ]
    }
   ],
   "source": [
    "modelMetricDic = {\"steps\": steps, \n",
    "                  \"lossVals\": lossVals, \n",
    "                  \"accVals\": accVals, \n",
    "                  \"accTest\": accTest}\n",
    "savedPath = \"../data/logisticRegressionModelMetrics.pickle\"\n",
    "with open(savedPath, 'wb') as handle:\n",
    "    pickle.dump(modelMetricDic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Model metrics saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
