{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape is (44, 20) and Test labels shape is (44,)\n",
      "Validation data shape is (44, 20) and Validation labels shape is (44,)\n",
      "Train data shape is (209, 20) and Train labels shape is (209,)\n"
     ]
    }
   ],
   "source": [
    "#Load preprocessed dataset\n",
    "savedPath = \"../data/splittedData.pickle\"\n",
    "\n",
    "with open(savedPath, \"rb\") as input_file:\n",
    "    dataDict = pickle.load(input_file)\n",
    "\n",
    "testData = dataDict[\"testData\"]\n",
    "testLabels = dataDict[\"testLabels\"]\n",
    "validationData = dataDict[\"validationData\"]\n",
    "validationLabels = dataDict[\"validationLabels\"]\n",
    "trainData = dataDict[\"trainData\"]\n",
    "trainLabels = dataDict[\"trainLabels\"]\n",
    "\n",
    "testLabels = testLabels.astype(np.float32)\n",
    "validationLabels = validationLabels.astype(np.float32)\n",
    "trainLabels = trainLabels.astype(np.float32)\n",
    "\n",
    "print(\"Test data shape is {} and Test labels shape is {}\".format(testData.shape, testLabels.shape))\n",
    "print(\"Validation data shape is {} and Validation labels shape is {}\"\n",
    "      .format(validationData.shape, validationLabels.shape))\n",
    "print(\"Train data shape is {} and Train labels shape is {}\".format(trainData.shape, trainLabels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL PARAMS\n",
    "NUMBER_OF_FEATURES = 20\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCH = 200\n",
    "CHECK_GAP = 100\n",
    "ALPHA_LASSO = 1.0\n",
    "ALPHA_RIDGE = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "xVals = tf.placeholder(shape=[None, NUMBER_OF_FEATURES], dtype=np.float32)\n",
    "yVals = tf.placeholder(shape=[None, 1], dtype=np.float32)\n",
    "\n",
    "weight = tf.Variable(tf.random_normal(shape=[NUMBER_OF_FEATURES, 1], mean=0.0, stddev=1))\n",
    "biais = tf.Variable(tf.random_normal(shape=[1, 1], mean=0.0, stddev=1))\n",
    "\n",
    "modelOutput = tf.add(tf.matmul(xVals, weight), biais)\n",
    "\n",
    "#Compute the loss\n",
    "weightAbs = tf.reduce_mean(tf.multiply(ALPHA_LASSO, tf.abs(weight)))\n",
    "weightSqrt = tf.reduce_mean(tf.multiply(ALPHA_RIDGE, tf.square(weight)))\n",
    "origLoss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=modelOutput, labels=yVals))\n",
    "loss = tf.add(tf.add(weightAbs, weightSqrt), origLoss)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "trainStep = optimizer.minimize(loss)\n",
    "\n",
    "#Compute Accuracy\n",
    "prediction = tf.round(tf.nn.sigmoid(modelOutput))\n",
    "correctPrediction = tf.cast(tf.equal(prediction, yVals), dtype=np.float32)\n",
    "accuracy = tf.reduce_mean(correctPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 100 VALIDATION: loss = 2.7079660892486572 and accuracy = 0.7045454382896423\n",
      "\n",
      "\n",
      "Step 200 VALIDATION: loss = 2.079659938812256 and accuracy = 0.75\n",
      "\n",
      "\n",
      "Step 300 VALIDATION: loss = 1.6187865734100342 and accuracy = 0.75\n",
      "\n",
      "\n",
      "Step 400 VALIDATION: loss = 1.277435064315796 and accuracy = 0.7272727489471436\n",
      "\n",
      "\n",
      "Step 500 VALIDATION: loss = 1.0285015106201172 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 600 VALIDATION: loss = 0.8550938963890076 and accuracy = 0.7727272510528564\n",
      "\n",
      "\n",
      "Step 700 VALIDATION: loss = 0.7429475784301758 and accuracy = 0.75\n",
      "\n",
      "\n",
      "Step 800 VALIDATION: loss = 0.6829133629798889 and accuracy = 0.7954545617103577\n",
      "\n",
      "\n",
      "Step 900 VALIDATION: loss = 0.6478911638259888 and accuracy = 0.8181818127632141\n",
      "\n",
      "\n",
      "Step 1000 VALIDATION: loss = 0.6260368227958679 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1100 VALIDATION: loss = 0.6093345284461975 and accuracy = 0.8181818127632141\n",
      "\n",
      "\n",
      "Step 1200 VALIDATION: loss = 0.5990113615989685 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1300 VALIDATION: loss = 0.5909672975540161 and accuracy = 0.8636363744735718\n",
      "\n",
      "\n",
      "Step 1400 VALIDATION: loss = 0.5859059691429138 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1500 VALIDATION: loss = 0.5811434388160706 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1600 VALIDATION: loss = 0.5783751010894775 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1700 VALIDATION: loss = 0.5762298107147217 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1800 VALIDATION: loss = 0.5737965106964111 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 1900 VALIDATION: loss = 0.5716208815574646 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2000 VALIDATION: loss = 0.5702358484268188 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2100 VALIDATION: loss = 0.5679144859313965 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2200 VALIDATION: loss = 0.566815197467804 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2300 VALIDATION: loss = 0.5657110214233398 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2400 VALIDATION: loss = 0.5666220188140869 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2500 VALIDATION: loss = 0.564328670501709 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2600 VALIDATION: loss = 0.5639026165008545 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2700 VALIDATION: loss = 0.5642507672309875 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2800 VALIDATION: loss = 0.5633171200752258 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 2900 VALIDATION: loss = 0.5636342763900757 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3000 VALIDATION: loss = 0.5632396936416626 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3100 VALIDATION: loss = 0.562455952167511 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3200 VALIDATION: loss = 0.5632445812225342 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3300 VALIDATION: loss = 0.5622883439064026 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3400 VALIDATION: loss = 0.5628868341445923 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3500 VALIDATION: loss = 0.5633500218391418 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3600 VALIDATION: loss = 0.5625874996185303 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3700 VALIDATION: loss = 0.5633516311645508 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3800 VALIDATION: loss = 0.5635731816291809 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 3900 VALIDATION: loss = 0.5617408156394958 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4000 VALIDATION: loss = 0.5627346038818359 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4100 VALIDATION: loss = 0.5617510080337524 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4200 VALIDATION: loss = 0.5620545744895935 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4300 VALIDATION: loss = 0.5612260699272156 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4400 VALIDATION: loss = 0.5615524053573608 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4500 VALIDATION: loss = 0.5615443587303162 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4600 VALIDATION: loss = 0.5619144439697266 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4700 VALIDATION: loss = 0.561884343624115 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4800 VALIDATION: loss = 0.5608590841293335 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 4900 VALIDATION: loss = 0.5624850988388062 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 5000 VALIDATION: loss = 0.5615418553352356 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 5100 VALIDATION: loss = 0.5620863437652588 and accuracy = 0.8409090638160706\n",
      "\n",
      "\n",
      "Step 5200 VALIDATION: loss = 0.5615595579147339 and accuracy = 0.8409090638160706\n",
      "\n",
      "ACCURACY ON TEST SET: 0.8409090638160706\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "steps = []\n",
    "lossVals = []\n",
    "accVals = []\n",
    "\n",
    "currentStep = 0\n",
    "trainDataSize = trainData.shape[0]\n",
    "np.random.seed(seed=18)\n",
    "\n",
    "for index in range(NUM_EPOCH) :\n",
    "    indexes = np.arange(trainDataSize)\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    highIndex = 0\n",
    "    while highIndex + BATCH_SIZE < trainDataSize :\n",
    "        batchIndexes = indexes[highIndex:highIndex + BATCH_SIZE]\n",
    "        highIndex = highIndex + BATCH_SIZE\n",
    "        \n",
    "        lss, _, acc = sess.run([loss, trainStep, accuracy], feed_dict={xVals: trainData[batchIndexes], \n",
    "                                                               yVals: trainLabels[batchIndexes].reshape((-1, 1))})    \n",
    "        currentStep = currentStep + 1\n",
    "#         print(\"Step {} Training: loss = {} and accuracy = {}\".format(currentStep, lss, acc))\n",
    "        \n",
    "        if currentStep % CHECK_GAP == 0 :\n",
    "            validationIndexes = np.arange(validationData.shape[0])\n",
    "            np.random.shuffle(validationIndexes)\n",
    "            lossVall, accVall = sess.run([loss, accuracy], feed_dict={xVals: validationData[validationIndexes], \n",
    "                                                               yVals: validationLabels[validationIndexes].reshape((-1, 1))})\n",
    "            steps.append(currentStep)\n",
    "            lossVals.append(lossVall)\n",
    "            accVals.append(accVall)\n",
    "            \n",
    "            print()\n",
    "            print(\"Step {} VALIDATION: loss = {} and accuracy = {}\".format(currentStep, lossVall, accVall))\n",
    "            print()\n",
    "\n",
    "#Accuracy on test set\n",
    "accTest = sess.run(accuracy, feed_dict={xVals: testData, yVals: testLabels.reshape((-1, 1))})\n",
    "print(\"ACCURACY ON TEST SET: {}\".format(accTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model metrics saved\n"
     ]
    }
   ],
   "source": [
    "modelMetricDic = {\"steps\": steps, \n",
    "                  \"lossVals\": lossVals, \n",
    "                  \"accVals\": accVals, \n",
    "                  \"accTest\": accTest}\n",
    "savedPath = \"../data/elasticNetRegressionModelMetrics.pickle\"\n",
    "with open(savedPath, 'wb') as handle:\n",
    "    pickle.dump(modelMetricDic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Model metrics saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
